[{"authors":["admin"],"categories":null,"content":"ZHANG Hao is a research engineer of Artificial Intelligence Initiative (A*AI) at Agency for Science, Technology and Research (A*STAR), Singapore. His research interests include natural language processing, visual grounding, reinforcement learning for robotics and machine learning methods.\nMeanwhile, ZHANG Hao is pursuing his Ph.D. in computer science at Nayang Technological University (NTU) since August 2019, and his supervisor is Associate Prof. Aixin SUN (NTU) and cosupervisor is Dr. Joey Tianyi ZHOU (A*STAR).\nEmail: 26hzhang AT gmail DOT com\nWHAT I KNOW\nProgramming Languages: Python, Java, C/C++, JavaScript.\nTools \u0026amp; Frameworks: TensorFlow, PyTorch, Keras, DL4J, Stanford NLP, Spark, ConceptNet, Neo4J and etc.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"ZHANG Hao is a research engineer of Artificial Intelligence Initiative (A*AI) at Agency for Science, Technology and Research (A*STAR), Singapore. His research interests include natural language processing, visual grounding, reinforcement learning for robotics and machine learning methods.\nMeanwhile, ZHANG Hao is pursuing his Ph.D. in computer science at Nayang Technological University (NTU) since August 2019, and his supervisor is Associate Prof. Aixin SUN (NTU) and cosupervisor is Dr. Joey Tianyi ZHOU (A*STAR).","tags":null,"title":"ZHANG Hao","type":"authors"},{"authors":["Hao Zhang","Aixin Sun","Wei Jing","Joey Tianyi Zhou"],"categories":null,"content":"","date":1590278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590278400,"objectID":"7d8d049b9297741401ed90a0ac7a70ac","permalink":"/publication/vslnet/","publishdate":"2020-05-24T00:00:00Z","relpermalink":"/publication/vslnet/","section":"publication","summary":"Given an untrimmed video and a text query, natural language video localization (NLVL) is to locate a matching span from the video that semantically corresponds to the query. Existing solutions formulate NLVL either as a ranking task and apply multimodal matching architecture, or as a regression task to directly regress the target video span. In this work, we address NLVL task with a span-based QA approach by treating the input video as text passage. We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework, to address NLVL. The proposed VSLNet tackles the differences between NLVL and span-based QA through a simple and yet effective query-guided highlighting (QGH) strategy. The QGH guides VSLNet to search for matching video span within a highlighted region. Through extensive experiments on three benchmark datasets, we show that the proposed VSLNet outperforms the state-of-the-art methods; and adopting span-based QA framework is a promising direction to solve NLVL.","tags":null,"title":"Span-based Localizing Network for Natural Language Video Localization","type":"publication"},{"authors":["Ming Yan","Hao Zhang","Di Jin","Joey Tianyi Zhou"],"categories":null,"content":"","date":1590192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590192000,"objectID":"7009f54c2228fd78a30ce3f72f0d41b8","permalink":"/publication/mmt/","publishdate":"2020-05-23T00:00:00Z","relpermalink":"/publication/mmt/","section":"publication","summary":"Multiple-choice question answering (MCQA) is one of the most challenging tasks in machine reading comprehension since it requires more advanced reading comprehension skills such as logical reasoning, summarization, and arithmetic operations. Unfortunately, most existing MCQA datasets are small in size, which increases the difficulty of model learning and generalization. To address this challenge, we propose a multi-source meta transfer (MMT) for low-resource MCQA. In this framework, we first extend meta learning by incorporating multiple training sources to learn a generalized feature representation across domains. To bridge the distribution gap between training sources and the target, we further introduce the meta transfer that can be integrated into the multi-source meta training. More importantly, the proposed MMT is independent of backbone language models. Extensive experiments demonstrate the superiority of MMT over state-of-the-arts, and continuous improvements can be achieved on different backbone networks on both supervised and unsupervised domain adaptation settings.","tags":null,"title":"Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering","type":"publication"},{"authors":["Tianying Wang*","Wei Qi Toh*","Hao Zhang*","Xiuchao Sui","Shaohua Li","Yong Liu","Wei Jing"],"categories":null,"content":"","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581033600,"objectID":"27c876932774e513bc2d0762f1067e33","permalink":"/publication/robocodraw/","publishdate":"2020-02-15T00:00:00Z","relpermalink":"/publication/robocodraw/","section":"publication","summary":"Robotic drawing has become increasingly popular as an entertainment and interactive tool. In this paper we present RoboCoDraw, a real-time collaborative robot-based drawing system that draws stylized human face sketches interactively in front of human users, by using the Generative Adversarial Network (GAN)-based style transfer and a Random-Key Genetic Algorithm (RKGA)-based path optimization. The proposed RoboCoDraw system takes a real human face image as input, converts it to a stylized avatar, then draws it with a robotic arm. A core component in this system is the Avatar-GAN proposed by us, which generates a cartoon avatar face image from a real human face. AvatarGAN is trained with unpaired face and avatar images only and can generate avatar images of much better likeness with human face images in comparison with the vanilla CycleGAN. After the avatar image is generated, it is fed to a line extraction algorithm and converted to sketches. An RKGA-based path optimization algorithm is applied to find a time-efficient robotic drawing path to be executed by the robotic arm. We demonstrate the capability of RoboCoDraw on various face images using a lightweight, safe collaborative robot UR5.","tags":null,"title":"RoboCoDraw: Robotic Avatar Drawing with GAN-based Style Transfer and Time-efficient Path Optimization","type":"publication"},{"authors":["Tianying Wang","Hao Zhang","Wei Qi Toh","Hongyuan Zhu","Cheston Tan","Yan Wu","Yong Liu","Wei Jing"],"categories":null,"content":"","date":1575590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575590400,"objectID":"409102bf0ed4b329fd29259c3e1faf4d","permalink":"/publication/deep-model-fusion/","publishdate":"2019-12-06T00:00:00Z","relpermalink":"/publication/deep-model-fusion/","section":"publication","summary":"Learning-based methods have been used to program robotic tasks in recent years. However, extensive training is usually required not only for the initial task learning but also for generalizing the learned model to the same task but in different environments. In this paper, we propose a novel Deep Reinforcement Learning algorithm for efficient task generalization and environment adaptation in the robotic task learning problem. The proposed method is able to efficiently generalize the previously learned task by model fusion to solve the environment adaptation problem. The proposed Deep Model Fusion (DMF) method reuses and combines the previously trained model to improve the learning efficiency and results. Besides, we also introduce a Multi-objective Guided Reward (MGR) shaping technique to further improve training efficiency. The proposed method was benchmarked with previous methods in various environments to validate its effectiveness.","tags":null,"title":"Efficient Robotic Task Generalization Using Deep Model Fusion Reinforcement Learning","type":"publication"},{"authors":["Joey Tianyi Zhou*","Hao Zhang*","Di Jing","Xi Peng"],"categories":null,"content":"","date":1564531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564531200,"objectID":"9f2b298f60364c5452911293fdb512a4","permalink":"/publication/datnet-tpami/","publishdate":"2019-07-31T00:00:00Z","relpermalink":"/publication/datnet-tpami/","section":"publication","summary":"We propose a new architecture for addressing sequence labeling, termed Dual Adversarial Transfer Network (DATNet). Specifically, the proposed DATNet includes two variants, i.e., DATNet-F and DATNet-P, which are proposed to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD) and adopt adversarial training to boost model generalization. We investigate the effects of different components of DATNet across different domains and languages, and show that significant improvement can be obtained especially for low-resource data. Without augmenting any additional hand-crafted features, we achieve state-of-the-art performances on CoNLL, Twitter, PTB-WSJ, OntoNotes and Universal Dependencies with three popular sequence labeling tasks, i.e. Named entity recognition (NER), Part-of-Speech (POS) Tagging and Chunking.","tags":null,"title":"Dual Adversarial Transfer for Sequence Labeling","type":"publication"},{"authors":["Joey Tianyi Zhou*","Hao Zhang*","Di Jing","Hongyuan Zhu","Rick Siow Mong Goh","Kenneth Kwok"],"categories":null,"content":"","date":1564444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564444800,"objectID":"71993f2b05043b69c161f86249fe19cb","permalink":"/publication/datnet/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/datnet/","section":"publication","summary":"We propose a new neural transfer method termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are investigated to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. In experiments, we examine the effects of different components in DATNet across domains and languages, and show that significant improvement can be obtained especially for low-resource data, without augmenting any additional hand-crafted features and pre-trained language model.","tags":null,"title":"Dual Adversarial Neural Transfer for Low-resource Named Entity Recognition","type":"publication"},{"authors":["Joey Tianyi Zhou*","Hao Zhang*","Di Jing","Xi Peng","Yang Xiao","Zhiguo Cao"],"categories":null,"content":"","date":1554508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554508800,"objectID":"a6ed9b952915cda58369c5d7c175fc14","permalink":"/publication/roseq/","publishdate":"2019-04-06T00:00:00Z","relpermalink":"/publication/roseq/","section":"publication","summary":"In this paper, we mainly investigate two issues for sequence labeling, namely label imbalance and noisy data which are commonly seen in the scenario of Named Entity Recognition and are largely ignored in existing works. To address these two issues, a new  method termed robust sequence labeling (RoSeq) is proposed. Specifically, to handle the label imbalance issue, we first incorporate label statistics in a novel CRF loss. Additionally, we design an additional loss to reduce the weights of overwhelming easy tokens for augmenting the CRF loss. To address the noisy training data, we adopt an adversarial training strategy to improve model generalization. In experiments, the proposed RoSeq achieves state-of-the-art performances on CoNLL and English Twitter NER benchmark datasets without using additional data.","tags":null,"title":"RoSeq: Robust Sequence Labeling","type":"publication"},{"authors":["Joey Tianyi Zhou","Meng Fang","Hao Zhang","Chen Gong","Xi Peng","Zhiguo Cao","Rick Siow Mong Goh"],"categories":null,"content":"","date":1547078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547078400,"objectID":"ca78f002450f9fba205ffdc6d4dfaa91","permalink":"/publication/lavd/","publishdate":"2019-01-10T00:00:00Z","relpermalink":"/publication/lavd/","section":"publication","summary":"In this paper, we study a new problem in the scenario of sequences labeling. To be exact, we consider that the training data are with annotation of various degrees, namely, fully labeled, unlabeled, and partially labeled sequences. The learning with fully un-/labeled sequence refers to the standard setting in traditional un-/supervised learning, and the proposed partially labeling specifies the subject that the element does not belong to. The partially labeled data are cheaper to obtain compared with the fully labeled data though it is less informative, especially, when the tasks require a lot of domain knowledge. To solve such a practical challenge, we propose a novel deep Conditional Random Field (CRF) model which utilizes an end-to-end learning manner to smoothly handle fully/un-/partially labeled sequences within a unified framework. To the best of our knowledge, this could be one of the first works to utilize the partially labeled instance for sequence labeling and the proposed algorithm unifies the deep learning and CRF in an end-to-end framework. Extensive experiments show that our method achieves state-of-the-art performance in two sequence labeling tasks on some popular data sets.","tags":null,"title":"Learning With Annotation of Various Degrees","type":"publication"},{"authors":null,"categories":null,"content":"Commonsense knowledge bases (KBs) are needed for inference in AI, in contexts such as natural language understanding, image and visual scene understanding, decision-making, etc. For tasks involving real-time interaction and decision-making, especially, the speed of such inference can be critical.\nThe goal of PrimeNet is to set out a framework for a commonsense KB that allows for efficient processing, in order to meet the demands of commonsense reasoning and, hence, support intelligent machine performance in real-world tasks. At the same time, the PrimeNet still provide access to a vast knowledge resource of concepts involving specific object instances.\n","date":1547078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547078400,"objectID":"d520343040c07db8a23715823bb1a2e4","permalink":"/project/primenet/","publishdate":"2019-01-10T00:00:00Z","relpermalink":"/project/primenet/","section":"project","summary":"A human-inspired framework for commonsense knowledge representation and reasoning.","tags":null,"title":"PrimeNet: Human-inspired Framework for Commonsense Knowledge Representation and Reasoning","type":"project"},{"authors":["Huminski Aliaksandr","Hao Zhang"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"dfa85e97818d1c5f70ba22fce818e735","permalink":"/publication/action-hierarchy/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/publication/action-hierarchy/","section":"publication","summary":"Modeling action as an important topic in robotics and human-computer communication assumes by default examining a large set of actions as described by natural language. We offer a procedure for how to extract actions from WordNet. It is based on the analysis of the whole set of verbs and includes 5 steps for implementation. The result is not just a set of extracted actions but a hierarchical structure. In the second part of the article, we describe how an action hierarchy can give an additional benefit in a representation of actions, in particular how it can improve an action representation through semantic roles.","tags":null,"title":"Action Hierarchy Extraction and its Application","type":"publication"},{"authors":null,"categories":null,"content":"Current video surveillance systems still require users to monitor an array of video streams from a myraid of cameras to pick out potential threats and other events of interest. Attemps to automate the process typically rely on hand-crafted rules which cannot handle dynamic, uncertain and comples siturations requiring new hand-crafted rules to address each and every new situation. In this project, we apply and extend behavioural learning and commonsense reasoning to video data analysis, which allow video surveillance systems to learn new behaviours without the need to hand craft rules, and to use the learnt behaviours to perform behavioural reasoning in order to predict downstream behaviours and recommend courses of action, and to generate narrative descriptions of visual scenes containing previously learnt behaviours.\nThe objective of the project, named MARACANA, is to develop technology components that can analyze and narrate real world events captured in video, and demonstrate these capabilities in a static surveillance scenario. The technology will provide the human user with a rapid understanding of the happenings in an environment. Generally, we focus on addressing the following issues:\n Develop components that are not built-in to work on only specific scenarios but are adaptive to various scenarios. Knowledge learnt in one scenario should also be transferrable to other scenarios to effect faster learning. Description of the events happening in the real world is in the form of textual natural language.  In order to tackle those problems, several technology components, including event / activity recognition and tracking, behaviour extraction, behavioural (causal) learning, commonsense reasoning/learning, behavioural (causal) reasoning and narrative scene description, are developed and then integrated to form the whole system. The general architecture is shown in the figure above, it briefly illustrates the overall workflow for training and opration processes.\n","date":1524182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524182400,"objectID":"db9ad1d5edab76f5d2b027393490ccff","permalink":"/project/maracana/","publishdate":"2018-04-20T00:00:00Z","relpermalink":"/project/maracana/","section":"project","summary":"An effectively and flexibly multimodal system to analyze and narrate real world events captured in video.","tags":null,"title":"MARACANA: Behavioural Understanding and Narrative Descriptions from Videos","type":"project"},{"authors":["Huminski Aliaksandr","Hao Zhang"],"categories":null,"content":"","date":1515369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515369600,"objectID":"5a40e3ae7c0838639957b5daec5f55f3","permalink":"/publication/wordnet/","publishdate":"2018-01-08T00:00:00Z","relpermalink":"/publication/wordnet/","section":"publication","summary":"Commonsense knowledge bases need to have relations that allow to predict the consequences of specific actions (say, if John stabbed Peter, Peter might be killed) and to unfold the possible actions for the specific results (Peter was killed. It could happen because of poisoning, stabbing, shooting, etc.) This kind of causal relations are established between manner verbs and result verbs: manner-result relations. We offer a procedure on how to extract manner-result relations from WordNet through the analysis of the troponym glosses. The procedure of extraction includes three steps and the results are based on the analysis of the whole set of verbs in WordNet.","tags":null,"title":"WordNet Troponymy and Extraction of Manner-Result Relations","type":"publication"},{"authors":null,"categories":null,"content":"Supervisor: Lap Pui Chau\nUnderwater vision enhancement via backscatter removing is widely used in ocean engineering. However, due to the existence of dust-like particles and light attenuation, underwater images and videos always suffer from the problems of low contrast and color distortion.\nIn this project, we investigated the underwater light propagation process from a physical standpoint, studied the state-of-the-art methods of sloving the problem and finally proposed a novel and effective method based on underwater optical model and fusion technique to overcome the backscatter problem.\nIn general, state-of-the-art strategies to solve this issue can be categorized into two parts, one is physical-based model, which studys its physical process and then reverse derivation, while another is more focus on image processing techniques. Experiment shows that carefully designed physical-based models often achieve excellent in specific situations, but are not able to generate great results for varieties of underwater environments all the time. By contrast, image processing methods are more flexiable and perform good under various environments, although they cannot always guarantee to give good contrast and texture information.\nIn order to solve the underwater imaging issue well and also take the universality of the model into consideration, we investigated to incorporate the merits of both physical-based model and image processing model, while suppress their drawbacks at the same time. Hence, we came up with a new technique which utilize a novel fusion strategy to fuse the restored results from physical mdoel and image processing model. Our proposed method is mainly consist of three steps:\n We decomposed the input image into two components, reflectance channel and illuminance channel; We utilized color correction technology and dehazing technology to handle these two components separately; Finally, in order to rebuild result well, we applied the Gaussian and Laplacian pyramids based multi-scale fusion to reconstruct the target image while exposedness, saliency maps are utilized as weights to assist the fusion task.  The experimental results show that the proposed method is able to greatly improve the quality of distorted underwater images. By introducing the underwater image quality metric measurements, we also analyze the intrinsic information and objective feature indexes of restored images via different methods. In general, our method outperforms state-of-the-arts among sets of test images captured in different water environments and is demonstrated to be well-performed and effective. ","date":1471305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1471305600,"objectID":"4f3758beffbbcb7eea4188933786427d","permalink":"/project/backscatter/","publishdate":"2016-08-16T00:00:00Z","relpermalink":"/project/backscatter/","section":"project","summary":"A novel and effective method based on underwater optical model for underwater object visibility enhancement.","tags":null,"title":"Removing Backscatter to Enhance the Visibility of Underwater Object","type":"project"},{"authors":["Hao Zhang","Lap Pui Chau"],"categories":null,"content":"","date":1471305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1471305600,"objectID":"e79ed7dd13f21e5955b5a07181e61c0e","permalink":"/publication/backscatter/","publishdate":"2016-08-16T00:00:00Z","relpermalink":"/publication/backscatter/","section":"publication","summary":"Underwater vision enhancement via backscatter removing is widely used in ocean engineering. With increasing ocean exploration, underwater image processing has drawn more and more attention due to the important roles of video and image for obtain information. However, due to the existence of dust-like particles and light attenuation, underwater images and videos always suffer from the problems of low contrast and color distortion. In this thesis, we analyze the underwater light propagation process and propose an effective method to overcome the backscatter problem. Our method is based on the underwater optical model and image fusion. It mainly contains three steps, first, we decompose input image into reflectance and illuminance components; second, we utilize color correction technology and dehazing technology to handle these two components separately; finally, in order to rebuild result well, we applied the Gaussian and Laplacian pyramids based multi-scale fusion to reconstruct the target image while exposedness, saliency maps are utilized as weights to assist the fusion task. The experimental results show that our proposed method is able to greatly improve the quality of distorted underwater images. By introducing the underwater image quality metric measurements, we also analyze the intrinsic information and objective feature indexes of restored images via different methods. In general, our proposed method outperforms state of the art among sets of test images captured in different water environments and is demonstrated to be well-performed and effective.","tags":null,"title":"Removing Backscatter to Enhance the Visibility of Underwater Object","type":"publication"}]