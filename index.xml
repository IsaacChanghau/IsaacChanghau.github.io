<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>IsaacChanghau</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>IsaacChanghau</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© IsaacChanghau · 2017-2020</copyright><lastBuildDate>Fri, 06 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>IsaacChanghau</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Efficient Robotic Task Generalization Using Deep Model Fusion Reinforcement Learning</title>
      <link>/publication/deep-model-fusion/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/publication/deep-model-fusion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dual Adversarial Transfer for Sequence Labeling</title>
      <link>/publication/datnet-tpami/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/publication/datnet-tpami/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dual Adversarial Neural Transfer for Low-resource Named Entity Recognition</title>
      <link>/publication/datnet/</link>
      <pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/publication/datnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>RoSeq: Robust Sequence Labeling</title>
      <link>/publication/roseq/</link>
      <pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/roseq/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning With Annotation of Various Degrees</title>
      <link>/publication/lavd/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/publication/lavd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PrimeNet: Human-inspired Framework for Commonsense Knowledge Representation and Reasoning</title>
      <link>/project/primenet/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/project/primenet/</guid>
      <description>&lt;p&gt;Commonsense knowledge bases (KBs) are needed for inference in AI, in contexts such as natural language understanding, image and visual scene understanding, decision-making, etc. For tasks involving real-time interaction and decision-making, especially, the speed of such inference can be critical.&lt;/p&gt;
&lt;p&gt;The goal of PrimeNet is to set out a framework for a commonsense KB that allows for efficient processing, in order to meet the demands of commonsense reasoning and, hence, support intelligent machine performance in real-world tasks. At the same time, the PrimeNet still provide access to a vast knowledge resource of concepts involving specific object instances.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Action Hierarchy Extraction and its Application</title>
      <link>/publication/action-hierarchy/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>/publication/action-hierarchy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MARACANA: Behavioural Understanding and Narrative Descriptions from Videos</title>
      <link>/project/maracana/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/project/maracana/</guid>
      <description>&lt;p&gt;Current video surveillance systems still require users to monitor an array of video streams from a myraid of cameras to pick out potential threats and other events of interest. Attemps to automate the process typically rely on hand-crafted rules which cannot handle dynamic, uncertain and comples siturations requiring new hand-crafted rules to address each and every new situation. In this project, we apply and extend behavioural learning and commonsense reasoning to video data analysis, which allow video surveillance systems to learn new behaviours without the need to hand craft rules, and to use the learnt behaviours to perform behavioural reasoning in order to predict downstream behaviours and recommend courses of action, and to generate narrative descriptions of visual scenes containing previously learnt behaviours.&lt;/p&gt;
&lt;p&gt;The objective of the project, named MARACANA, is to develop technology components that can analyze and narrate real world events captured in video, and demonstrate these capabilities in a static surveillance scenario. The technology will provide the human user with a rapid understanding of the happenings in an environment. Generally, we focus on addressing the following issues:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Develop components that are not built-in to work on only specific scenarios but are adaptive to various scenarios.&lt;/li&gt;
&lt;li&gt;Knowledge learnt in one scenario should also be transferrable to other scenarios to effect faster learning.&lt;/li&gt;
&lt;li&gt;Description of the events happening in the real world is in the form of textual natural language.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to tackle those problems, several technology components, including &lt;strong&gt;event / activity recognition and tracking&lt;/strong&gt;, &lt;strong&gt;behaviour extraction&lt;/strong&gt;, &lt;strong&gt;behavioural (causal) learning&lt;/strong&gt;, &lt;strong&gt;commonsense reasoning/learning&lt;/strong&gt;, &lt;strong&gt;behavioural (causal) reasoning&lt;/strong&gt; and &lt;strong&gt;narrative scene description&lt;/strong&gt;, are developed and then integrated to form the whole system. The general architecture is shown in the figure below, it briefly illustrates the overall workflow for training and opration processes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Sequence Labeling</title>
      <link>/project/seqlabeling/</link>
      <pubDate>Tue, 20 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/project/seqlabeling/</guid>
      <description>&lt;p&gt;Sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values. It includes but not limit to Part Of Speech (POS) tagging, Chunking, Named Entity Recognition (NER), Semantic Role Labeling (SRL) and Punctuation Restoration. Those tasks can be treated as the pre-processing for various natural language processing tasks, which are capable of providing aplenty syntatic or semantic information and greatly improving the performance of subsequent tasks.&lt;/p&gt;
&lt;p&gt;In this project, we investigated a unified neural network architecture and learning algorithm that can be applied to various sequence labeling tasks including Part of Speech (POS) Tagging, Chunking, Named Entity Recognition (NER) and so on. The aim of this project is to build an effective and versatile module that is able to learn and performance well among various seqeuence labeling datasets without exploiting man-made input features, carefully tuning parameters or setting up different model structures. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge.&lt;/p&gt;
&lt;p&gt;We did experiments on different publicly available sequence labeling datasets, like 
&lt;a href=&#34;https://github.com/teropa/nlp/tree/master/resources/corpora/conll2002&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CoNLL2002 shared task dataset&lt;/a&gt;
, 
&lt;a href=&#34;https://cogcomp.org/page/resource_view/81&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CoNLL2003 shared task dataset&lt;/a&gt;
, 
&lt;a href=&#34;http://hltc.cs.ust.hk/iwslt/index.php/evaluation-campaign/ted-task.html#MTtrack&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IWSLT dataset&lt;/a&gt;
, 
&lt;a href=&#34;https://github.com/EuropeanaNewspapers/ner-corpora&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Europeana Newspapers dataset&lt;/a&gt;
 and etc., which include POS tagging, chunking, NER and Punctuation Restoration tasks, and achieve convincing results, which are comparable or better than the the state-of-the-arts with carefully hyperparameters tuning and configuration modifying.&lt;/p&gt;
&lt;p&gt;Generally, our model follows the structure of 
&lt;a href=&#34;https://arxiv.org/pdf/1603.01354.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;BiRNN-CNNs-CRF&lt;/code&gt;&lt;/a&gt;
 with some modifications, as well as additional and optional modules, such as &lt;code&gt;highway network&lt;/code&gt;, &lt;code&gt;attention mechanism&lt;/code&gt; and &lt;code&gt;residual connections&lt;/code&gt;. For example, we use the four layers structure for Punctuation Restoration task, which contains character-level CNN and word embeddings with highway layer for optimization (&lt;code&gt;1st layer&lt;/code&gt;), 4-layer 
&lt;a href=&#34;https://github.com/IsaacChanghau/Dense_BiLSTM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;densely conntected bidirectional LSTM&lt;/a&gt;
 network to encode past and future contextual information for feature representations (&lt;code&gt;2nd layer&lt;/code&gt;), unidirectional LSTM layer with 
&lt;a href=&#34;https://arxiv.org/pdf/1409.0473.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;attention mechanism&lt;/a&gt;
 to learn more informative and contextual feature representations (&lt;code&gt;3rd layer&lt;/code&gt;) and a 
&lt;a href=&#34;http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;conditional random field (CRF) layer&lt;/a&gt;
 to decode the outputs (&lt;code&gt;4th layer&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details and source codes&lt;/strong&gt;: 
&lt;a href=&#34;https://github.com/IsaacChanghau/neural_sequence_labeling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IsaacChanghau/neural_sequence_labeling&lt;/a&gt;
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>WordNet Troponymy and Extraction of Manner-Result Relations</title>
      <link>/publication/wordnet/</link>
      <pubDate>Mon, 08 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/publication/wordnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Removing Backscatter to Enhance the Visibility of Underwater Object</title>
      <link>/project/backscatter/</link>
      <pubDate>Tue, 16 Aug 2016 00:00:00 +0000</pubDate>
      <guid>/project/backscatter/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Supervisor&lt;/strong&gt;: &lt;a href=&#34;http://www.ntu.edu.sg/home/elpchau/&#34; target=&#34;_blank&#34;&gt;Lap Pui Chau&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Underwater vision enhancement via backscatter removing is widely used in ocean engineering. However, due to the existence of dust-like particles and light attenuation, underwater images and videos always suffer from the problems of low contrast and color distortion.&lt;/p&gt;
&lt;p&gt;In this project, we investigated the underwater light propagation process from a physical standpoint, studied the state-of-the-art methods of sloving the problem and finally proposed a novel and effective method based on underwater optical model and fusion technique to overcome the backscatter problem.&lt;/p&gt;
&lt;p&gt;In general, state-of-the-art strategies to solve this issue can be categorized into two parts, one is physical-based model, which studys its physical process and then reverse derivation, while another is more focus on image processing techniques. Experiment shows that carefully designed physical-based models often achieve excellent in specific situations, but are not able to generate great results for varieties of underwater environments all the time. By contrast, image processing methods are more flexiable and perform good under various environments, although they cannot always guarantee to give good contrast and texture information.&lt;/p&gt;
&lt;p&gt;In order to solve the underwater imaging issue well and also take the universality of the model into consideration, we investigated to incorporate the merits of both physical-based model and image processing model, while suppress their drawbacks at the same time. Hence, we came up with a new technique which utilize a novel fusion strategy to fuse the restored results from physical mdoel and image processing model. Our proposed method is mainly consist of three steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We decomposed the input image into two components, reflectance channel and illuminance channel;&lt;/li&gt;
&lt;li&gt;We utilized color correction technology and dehazing technology to handle these two components separately;&lt;/li&gt;
&lt;li&gt;Finally, in order to rebuild result well, we applied the Gaussian and Laplacian pyramids based multi-scale fusion to reconstruct the target image while exposedness, saliency maps are utilized as weights to assist the fusion task.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The experimental results show that the proposed method is able to greatly improve the quality of distorted underwater images. By introducing the underwater image quality metric measurements, we also analyze the intrinsic information and objective feature indexes of restored images via different methods. In general, our method outperforms state-of-the-arts among sets of test images captured in different water environments and is demonstrated to be well-performed and effective.
&lt;img src=&#34;/img/backscatter.png&#34; alt=&#34;sample&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Removing Backscatter to Enhance the Visibility of Underwater Object</title>
      <link>/publication/backscatter/</link>
      <pubDate>Tue, 16 Aug 2016 00:00:00 +0000</pubDate>
      <guid>/publication/backscatter/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Image Enhancement based on Retinex Theory and Dual-tree Complex Wavelet Transform</title>
      <link>/project/retinex/</link>
      <pubDate>Fri, 01 May 2015 00:00:00 +0000</pubDate>
      <guid>/project/retinex/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Supervisor&lt;/strong&gt;: &lt;a href=&#34;http://faculty.dlut.edu.cn/MMCL_WHY/zh_CN/index.htm&#34; target=&#34;_blank&#34;&gt;Hongyu Wang&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The task of image enhancement is focus on restoring and clarifying the corrupted images to improve their quality, and image enhancement methods has been widely applied to numerous image analysis techniques including pattern recognition, image fusion, image segmentation, image compression and so forth. Among variety of image enhancement methods, algorithms based on Retinex theory have received more and more attentions and have been commonly used in many applications.&lt;/p&gt;
&lt;p&gt;In this project, we described a Retinex theory based method for contrast and illuminance enhancement in images of low light or unevenly illuminated scenes. This method firstly transforms image from RGB color space to HSV color space, and decomposes the value channel using dual-tree complex wavelet transform. Then, an improved adaptive local tone mapping method is used to process the low frequency component of the image, and wavelet shrinkage method and fuzzy enhancement method are applied to denoise and enhance the high frequency component of the image. After that, the enhanced value channel is reconstructed and a statistical histogram optimization method is used. Finally, the enhanced image is transformed back to RGB color space.&lt;/p&gt;
&lt;p&gt;In order to verify the feasibility and effectiveness of this method in image enhancement, this paper introduces different image quality assessment criterion, and conducts a great deal of experiments run by MATLAB, and compares against related image enhancement methods proposed by other scholars. Experiment results show that the method proposed by this paper performs very well with enhancement and denoise of the corrupted images.
&lt;img src=&#34;/img/retinex-sample.png&#34; alt=&#34;sample&#34;&gt;
Above shows the performance of proposed method (the last one), other state-of-the-arts and base-line methods. Left side shows the general performance while the right sides give details about local texture information restoration.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
