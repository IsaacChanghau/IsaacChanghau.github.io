<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | IsaacChanghau</title>
    <link>/project/</link>
      <atom:link href="/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© IsaacChanghau · 2017-2021</copyright><lastBuildDate>Wed, 20 Mar 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu1b364a951019a703f13d3cf900b79a27_31514_512x512_fill_lanczos_center_2.png</url>
      <title>Projects</title>
      <link>/project/</link>
    </image>
    
    <item>
      <title>Human-Robot Collaborative Al for Advanced Manufacturing and Engineering (AME)</title>
      <link>/project/hctl/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/project/hctl/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Work Package 3 Human-like Concept and Task Learning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Collab AI Programme seeks to enable a fundamental shift in human-machine interaction to allow intelligent machines to work alongside humans as partners, interacting in a natural human-like manner. WP3 will develop concept and task learning algorithms for cobots to learn skills and actions quickly and in a human-like manner, in order to achieve easy to use cobots system and better human-robot interactions. The algorithms should most focus on the Advanced Manufacturing Engineering (AME) related applications, or could be extended to certain part of the manufacturing processes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/hctl_pipeline.png&#34; alt=&#34;hctl&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Task Learning&lt;/strong&gt;: Learn robotic skills and actions through Reinforcement Learning (RL) and Planning. The hybrid modelling are proposed to use the planning to generate a baseline and use RL to improve around the baseline.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Concept Learning&lt;/strong&gt;: Implicitly learn the concept involved in the manufacturing applications. Unsupervised learning is used to ground the concepts in a human-like manner, such that those concepts could be used in the cognitive architecture, and for improving the robotic task learning as well.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/img/hctl.png&#34; alt=&#34;hctl&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PrimeNet: Human-inspired Framework for Commonsense Knowledge Representation and Reasoning</title>
      <link>/project/primenet/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/project/primenet/</guid>
      <description>&lt;p&gt;Commonsense knowledge bases (KBs) are needed for inference in AI, in contexts such as natural language understanding, image and visual scene understanding, decision-making, etc. For tasks involving real-time interaction and decision-making, especially, the speed of such inference can be critical.&lt;/p&gt;
&lt;p&gt;The goal of PrimeNet is to set out a framework for a commonsense KB that allows for efficient processing, in order to meet the demands of commonsense reasoning and, hence, support intelligent machine performance in real-world tasks. At the same time, the PrimeNet still provide access to a vast knowledge resource of concepts involving specific object instances.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MARACANA: Behavioural Understanding and Narrative Descriptions from Videos</title>
      <link>/project/maracana/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/project/maracana/</guid>
      <description>&lt;p&gt;Current video surveillance systems still require users to monitor an array of video streams from a myraid of cameras to pick out potential threats and other events of interest. Attemps to automate the process typically rely on hand-crafted rules which cannot handle dynamic, uncertain and comples siturations requiring new hand-crafted rules to address each and every new situation. In this project, we apply and extend behavioural learning and commonsense reasoning to video data analysis, which allow video surveillance systems to learn new behaviours without the need to hand craft rules, and to use the learnt behaviours to perform behavioural reasoning in order to predict downstream behaviours and recommend courses of action, and to generate narrative descriptions of visual scenes containing previously learnt behaviours.&lt;/p&gt;
&lt;p&gt;The objective of the project, named MARACANA, is to develop technology components that can analyze and narrate real world events captured in video, and demonstrate these capabilities in a static surveillance scenario. The technology will provide the human user with a rapid understanding of the happenings in an environment. Generally, we focus on addressing the following issues:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Develop components that are not built-in to work on only specific scenarios but are adaptive to various scenarios.&lt;/li&gt;
&lt;li&gt;Knowledge learnt in one scenario should also be transferrable to other scenarios to effect faster learning.&lt;/li&gt;
&lt;li&gt;Description of the events happening in the real world is in the form of textual natural language.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to tackle those problems, several technology components, including &lt;strong&gt;event / activity recognition and tracking&lt;/strong&gt;, &lt;strong&gt;behaviour extraction&lt;/strong&gt;, &lt;strong&gt;behavioural (causal) learning&lt;/strong&gt;, &lt;strong&gt;commonsense reasoning/learning&lt;/strong&gt;, &lt;strong&gt;behavioural (causal) reasoning&lt;/strong&gt; and &lt;strong&gt;narrative scene description&lt;/strong&gt;, are developed and then integrated to form the whole system. The general architecture is shown in the figure above, it briefly illustrates the overall workflow for training and opration processes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Removing Backscatter to Enhance the Visibility of Underwater Object</title>
      <link>/project/backscatter/</link>
      <pubDate>Tue, 16 Aug 2016 00:00:00 +0000</pubDate>
      <guid>/project/backscatter/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Supervisor&lt;/strong&gt;: &lt;a href=&#34;http://www.ntu.edu.sg/home/elpchau/&#34; target=&#34;_blank&#34;&gt;Lap Pui Chau&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Underwater vision enhancement via backscatter removing is widely used in ocean engineering. However, due to the existence of dust-like particles and light attenuation, underwater images and videos always suffer from the problems of low contrast and color distortion.&lt;/p&gt;
&lt;p&gt;In this project, we investigated the underwater light propagation process from a physical standpoint, studied the state-of-the-art methods of sloving the problem and finally proposed a novel and effective method based on underwater optical model and fusion technique to overcome the backscatter problem.&lt;/p&gt;
&lt;p&gt;In general, state-of-the-art strategies to solve this issue can be categorized into two parts, one is physical-based model, which studys its physical process and then reverse derivation, while another is more focus on image processing techniques. Experiment shows that carefully designed physical-based models often achieve excellent in specific situations, but are not able to generate great results for varieties of underwater environments all the time. By contrast, image processing methods are more flexiable and perform good under various environments, although they cannot always guarantee to give good contrast and texture information.&lt;/p&gt;
&lt;p&gt;In order to solve the underwater imaging issue well and also take the universality of the model into consideration, we investigated to incorporate the merits of both physical-based model and image processing model, while suppress their drawbacks at the same time. Hence, we came up with a new technique which utilize a novel fusion strategy to fuse the restored results from physical mdoel and image processing model. Our proposed method is mainly consist of three steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We decomposed the input image into two components, reflectance channel and illuminance channel;&lt;/li&gt;
&lt;li&gt;We utilized color correction technology and dehazing technology to handle these two components separately;&lt;/li&gt;
&lt;li&gt;Finally, in order to rebuild result well, we applied the Gaussian and Laplacian pyramids based multi-scale fusion to reconstruct the target image while exposedness, saliency maps are utilized as weights to assist the fusion task.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The experimental results show that the proposed method is able to greatly improve the quality of distorted underwater images. By introducing the underwater image quality metric measurements, we also analyze the intrinsic information and objective feature indexes of restored images via different methods. In general, our method outperforms state-of-the-arts among sets of test images captured in different water environments and is demonstrated to be well-performed and effective.
&lt;img src=&#34;/img/backscatter.png&#34; alt=&#34;sample&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
